{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literal Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Leung\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [colors_overview.ipynb](colors_overview.ipynb) for set-up in instructions and other background details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_color_selector import (\n",
    "    ColorizedNeuralListener, create_example_dataset)\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All two-word examples as a dev corpus\n",
    "\n",
    "So that you don't have to sit through excessively long training runs during development, I suggest working with the two-word-only subset of the corpus until you enter into the late stages of system testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME, \n",
    "    word_count=None, \n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = list(dev_corpus.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subset has about one-third the examples of the full corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_examples = [example for example in dev_examples if len(example.contents.split(\" \")) > 14]\n",
    "#for example in dev_examples:\n",
    "#    print(example.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev dataset\n",
    "\n",
    "The first step is to extract the raw color and raw texts from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols, dev_texts = zip(*[[ex.colors, ex.parse_turns()] for ex in dev_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw color representations are suitable inputs to a model, but the texts are just strings, so they can't really be processed as-is. Question 1 asks you to do some tokenizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing turns with token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The darker blue one', 'purple', 'Medium pink#the medium dark one', 'lime', 'Mint green.', 'Mud brown', 'Mud brown', 'Camo green', 'Darkish red', 'Grey']\n"
     ]
    }
   ],
   "source": [
    "dev_texts = [\"#\".join(text) for text in dev_texts]\n",
    "print(dev_texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random train–test split for development\n",
    "\n",
    "For the sake of development runs, we create a random train–test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols_train, dev_rawcols_test, dev_texts_train, dev_texts_test, dev_examples_train, dev_examples_test = \\\n",
    "    train_test_split(dev_rawcols, dev_texts, dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_rawcols_test, dev_rawcols_dev, dev_texts_test, dev_texts_tdev = \\\n",
    "#    train_test_split(dev_rawcols_test, dev_texts_test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35245\n",
      "11749\n"
     ]
    }
   ],
   "source": [
    "# Train=75%, dev=5%, test=20%\n",
    "print(len(dev_rawcols_train))\n",
    "print(len(dev_rawcols_test))\n",
    "#print(len(dev_rawcols_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors_utils import heuristic_ending_tokenizer\n",
    "\n",
    "def tokenize_example(s):\n",
    "    \n",
    "    # Improve me!\n",
    "    \n",
    "    return [START_SYMBOL] + heuristic_ending_tokenizer(s) + [END_SYMBOL]\n",
    "\n",
    "def clean_test_and_training(dev_seqs_train, dev_seqs_test):\n",
    "    # This method cleans the test set with $UNK, for those words that do not show up in the training set\n",
    "    vocab = {}\n",
    "    for toks in dev_seqs_train+dev_seqs_test:\n",
    "        for w in toks:\n",
    "            if w not in vocab:\n",
    "                vocab[w]=0\n",
    "            vocab[w]+=1\n",
    "    removal_candidates = {k:v for k, v in vocab.items() if v == 1 }\n",
    "    \n",
    "    dev_seqs_train = [[w if w not in removal_candidates else UNK_SYMBOL for w in toks] for toks in dev_seqs_train]\n",
    "\n",
    "    dev_seqs_test = [[w if w not in removal_candidates else UNK_SYMBOL for w in toks] for toks in dev_seqs_test]\n",
    "    return dev_seqs_train, dev_seqs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'aqua', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example(dev_texts_train[376])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tokenizer is working, run the following cell to tokenize your inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_seqs_train = [tokenize_example(s) for s in dev_texts_train]\n",
    "\n",
    "dev_seqs_test = [tokenize_example(s) for s in dev_texts_test]\n",
    "\n",
    "#dev_seqs_dev = [tokenize_example(s) for s in dev_texts_test]\n",
    "\n",
    "dev_seqs_train, dev_seqs_test = clean_test_and_training(dev_seqs_train, dev_seqs_test)\n",
    "\n",
    "#_, dev_seqs_dev = clean_test_and_training(dev_seqs_train, dev_seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only the train set to derive a vocabulary for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab = sorted({w for toks in dev_seqs_train for w in toks}) + [UNK_SYMBOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that the `UNK_SYMBOL` is included somewhere in this list. Test examples with word not seen in training will be mapped to `UNK_SYMBOL`. If you model's vocab is the same as your train vocab, then `UNK_SYMBOL` will never be encountered during training, so it will be a random vector at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2806"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a different split for the Speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should ensure that the speaker and the listener train on different datasets to prevent one from directly impling the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dev_seqs_train)\n",
    "dev_seqs_train_listener, dev_seqs_train_speaker = \\\n",
    "    dev_seqs_train[:n//2], dev_seqs_train[n//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab_listener = sorted({w for toks in dev_seqs_train_listener for w in toks}) + [UNK_SYMBOL]\n",
    "dev_vocab_speaker = sorted({w for toks in dev_seqs_train_speaker for w in toks}) + [UNK_SYMBOL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 1964\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_vocab_listener), len(dev_vocab_speaker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the color representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def represent_color_context(colors):\n",
    "    \n",
    "    # Improve me!\n",
    "    \n",
    "    return [represent_color(color) for color in colors]\n",
    "\n",
    "\n",
    "def represent_color(color):\n",
    "    import numpy.fft as fft\n",
    "    # Improve me!\n",
    "    #return color\n",
    "    #return colorsys.rgb_to_hsv(*color)\n",
    "    return fft.fft(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.07833333+0.j        , 0.24833333+0.19052559j,\n",
       "        0.24833333-0.19052559j]),\n",
       " array([ 0.88 +0.j        , -0.215-0.23382686j, -0.215+0.23382686j]),\n",
       " array([1.145+0.j        , 0.29 -0.37239092j, 0.29 +0.37239092j])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "represent_color_context(dev_rawcols_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the color representer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell just runs your `represent_color_context` on the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols_train = [represent_color_context(colors) for colors in dev_rawcols_train]\n",
    "\n",
    "dev_cols_test = [represent_color_context(colors) for colors in dev_rawcols_test]\n",
    "\n",
    "#dev_cols_dev = [represent_color_context(colors) for colors in dev_rawcols_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our preprocessing steps are complete, and we can fit a first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols_train_listener, dev_cols_train_speaker = \\\n",
    "    dev_cols_train[:n//2], dev_cols_train[n//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: GloVe embeddings [1 points]\n",
    "\n",
    "The above model uses a random initial embedding, as configured by the decoder used by `ContextualColorDescriber`. This homework question asks you to consider using GloVe inputs. \n",
    "\n",
    "__Your task__: Complete `create_glove_embedding` so that it creates a GloVe embedding based on your model vocabulary. This isn't mean to be analytically challenging, but rather just to create a basis for you to try out other kinds of rich initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_embedding(vocab, glove_base_filename='glove.6B.100d.txt'):\n",
    "    \n",
    "    # Use `utils.glove2dict` to read in the GloVe file:    \n",
    "    ##### YOUR CODE HERE\n",
    "    glove_dict = utils.glove2dict(os.path.join(GLOVE_HOME, glove_base_filename))\n",
    "\n",
    "    \n",
    "    # Use `utils.create_pretrained_embedding` to create the embedding.\n",
    "    # This function will, by default, ensure that START_TOKEN, \n",
    "    # END_TOKEN, and UNK_TOKEN are included in the embedding.\n",
    "    ##### YOUR CODE HERE\n",
    "    embedding, new_vocab = utils.create_pretrained_embedding(glove_dict, vocab)\n",
    "\n",
    "    \n",
    "    # Be sure to return the embedding you create as well as the\n",
    "    # vocabulary returned by `utils.create_pretrained_embedding`,\n",
    "    # which is likely to have been modified from the input `vocab`.\n",
    "    \n",
    "    ##### YOUR CODE HERE\n",
    "    return embedding, new_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the GloVe representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_glove_embedding, dev_glove_vocab = create_glove_embedding(dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_glove_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle():\n",
    "    import pickle \n",
    "\n",
    "    with open('dev_vocab.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_vocab_listener.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_vocab_listener, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_vocab_speaker.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_vocab_speaker, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_seqs_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_seqs_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_seqs_train.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_seqs_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_seqs_train_speaker.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_seqs_train_speaker, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_seqs_train_listener.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_seqs_train_listener, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_cols_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_cols_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_cols_train.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_cols_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_cols_train_speaker.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_cols_train_speaker, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_cols_train_listener.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_cols_train_listener, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_examples_train.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_examples_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_examples_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_examples_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('embedding.pickle', 'wb') as handle:\n",
    "        pickle.dump(embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_glove_vocab.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_glove_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('dev_glove_embedding.pickle', 'wb') as handle:\n",
    "        pickle.dump(dev_glove_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "save_to_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
