{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Leung\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [colors_overview.ipynb](colors_overview.ipynb) for set-up in instructions and other background details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_color_selector import (\n",
    "    ColorizedNeuralListener, create_example_dataset)\n",
    "from torch_listener_with_attention import (\n",
    "    AttentionalColorizedNeuralListener)\n",
    "from torch_color_describer import ColorizedInputDescriber\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All two-word examples as a dev corpus\n",
    "\n",
    "So that you don't have to sit through excessively long training runs during development, I suggest working with the two-word-only subset of the corpus until you enter into the late stages of system testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME, \n",
    "    word_count=None, \n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = list(dev_corpus.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subset has about one-third the examples of the full corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We __should__ worry that it's not a fully representative sample. Most of the descriptions in the full corpus are shorter, and a large proportion are longer. So this dataset is mainly for debugging, development, and general hill-climbing. All findings should be validated on the full dataset at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev dataset\n",
    "\n",
    "Let's load the saved training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle():\n",
    "    import pickle \n",
    "    \n",
    "    with open('dev_vocab.pickle', 'rb') as handle:\n",
    "        dev_vocab = pickle.load(handle)\n",
    "    with open('dev_seqs_test.pickle', 'rb') as handle:\n",
    "        dev_seqs_test = pickle.load(handle)\n",
    "    with open('dev_seqs_train.pickle', 'rb') as handle:\n",
    "        dev_seqs_train = pickle.load(handle)\n",
    "    with open('dev_cols_test.pickle', 'rb') as handle:\n",
    "        dev_cols_test = pickle.load(handle)\n",
    "    with open('dev_cols_train.pickle', 'rb') as handle:\n",
    "        dev_cols_train = pickle.load(handle)\n",
    "    with open('embedding.pickle', 'rb') as handle:\n",
    "        embedding = pickle.load(handle)\n",
    "    return dev_vocab, dev_seqs_test, dev_seqs_train, dev_cols_test, dev_cols_train, embedding\n",
    "dev_vocab, dev_seqs_test, dev_seqs_train, dev_cols_test, dev_cols_train, embedding = load_from_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our preprocessing steps are complete, and we can fit a first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embeddings\n",
    "\n",
    "We also load the GloVe embedding that was used by the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_pickle():\n",
    "    import pickle \n",
    "    with open('dev_glove_vocab.pickle', 'rb') as handle:\n",
    "        dev_glove_vocab = pickle.load(handle)\n",
    "    with open('dev_glove_embedding.pickle', 'rb') as handle:\n",
    "        dev_glove_embedding = pickle.load(handle)\n",
    "    return dev_glove_vocab, dev_glove_embedding\n",
    "dev_glove_vocab, dev_glove_embedding = load_glove_from_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above might dramatically change your vocabulary, depending on how many items from your vocab are in the Glove space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Literal Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "literal_listener = AttentionalColorizedNeuralListener(\n",
    "    dev_vocab, \n",
    "    #embedding=dev_glove_embedding, \n",
    "    embed_dim=100,\n",
    "    embedding=embedding,\n",
    "    hidden_dim=100, \n",
    "    max_iter=100,\n",
    "    batch_size=256,\n",
    "    dropout_prob=0.,\n",
    "    eta=0.001,\n",
    "    lr_rate=0.96,\n",
    "    warm_start=True,\n",
    "    device='cuda')\n",
    "literal_listener.load_model(\"literal_listener_with_attention.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_selector.py:77: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n"
     ]
    }
   ],
   "source": [
    "test_preds = literal_listener.predict(dev_cols_test, dev_seqs_test)\n",
    "train_preds = literal_listener.predict(dev_cols_train, dev_seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 9405 / 11749 0.8004936590348115\n",
      "train 31783 / 35245 0.901773301177472\n"
     ]
    }
   ],
   "source": [
    "correct = sum([1 if x == 2 else 0 for x in test_preds])\n",
    "print(\"test\", correct, \"/\", len(test_preds), correct/len(test_preds))\n",
    "correct = sum([1 if x == 2 else 0 for x in train_preds])\n",
    "print(\"train\", correct, \"/\", len(train_preds), correct/len(train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Literal Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "literal_speaker = ColorizedInputDescriber(\n",
    "    dev_glove_vocab, \n",
    "    embedding=dev_glove_embedding, \n",
    "    hidden_dim=100, \n",
    "    max_iter=40, \n",
    "    eta=0.005,\n",
    "    batch_size=128)\n",
    "literal_speaker.load_model(\"literal_speaker.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:70: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n",
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:677: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n"
     ]
    }
   ],
   "source": [
    "literal_speaker.listener_accuracy(dev_cols_test, dev_seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Pragmatic Speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we sample candidate utterances from the literal speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_samples = 8\n",
    "utterances = literal_speaker.sample_utterances(dev_cols_test, k_samples=k_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterances[262])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed these utterances to the literal listener to generate a score per utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_utterances = [seq for seq_list in utterances for seq in seq_list]\n",
    "input_col = [item for item in dev_cols_test for i in range(k_samples)]\n",
    "utterance_preds = literal_listener.predict(input_col, target_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the accuracy of these utterances (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum([1 if x == 2 else 0 for x in utterance_preds])\n",
    "print(\"utterance acc:\", correct, \"/\", len(utterance_preds), correct/len(utterance_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above accuracy shouldn't be reflective of the actual performance of the speaker and listener."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the utterance predictions, we calculate it and this time, we get the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_probs = literal_listener.predict(input_col, target_utterances, probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterance_probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these predictions, we only need the prediction of the target color. This will serve as $P(t | u, C;\\theta)$, where $t$ is the color, $u$ is the utterance, $C$ is a color context, and $\\theta$ is learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_probs = torch.FloatTensor([preds[2] for preds in utterance_probs]).view(-1, k_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_probs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then scale this by an alpha to control the degree of pragmaticism of the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.544\n",
    "utterance_probs = utterance_probs ** alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, there are 5 utterances per color, if we normalize these we will get the pragmatic speaker's likelihood per sampled utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = torch.sum(utterance_probs, dim=1).unsqueeze(1)\n",
    "normalized_utterance_probs = utterance_probs/total\n",
    "print(normalized_utterance_probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the index for the best utterances with the highest probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flip first since argmax takes the last index to break ties.\n",
    "best_utter_index = torch.argmax(normalized_utterance_probs.flip(dims=[1]), dim=1)\n",
    "# Then flip the index number back.\n",
    "prag_speaker_pred_ind = normalized_utterance_probs.shape[1] - best_utter_index - 1\n",
    "print(prag_speaker_pred_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with the indices, we get the pragmatic speaker utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prag_speaker_pred = [seqs[prag_speaker_pred_ind[ind]] for ind, seqs in enumerate(utterances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prag_speaker_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we re-benchmark with the listener. We should have a fairly high listening accuracy. This is because we do an argmax over the literal listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_preds = literal_listener.predict(dev_cols_test, prag_speaker_pred)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(listened_preds):\n",
    "    if x != 2:\n",
    "        print(prag_speaker_pred[idx], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy here is extremely high. This is proof that the pragmatic speaker is expressive enough to maximize communication with the literal listener. Let's compare this with the literal speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_preds = literal_listener.predict(dev_cols_test, literal_speaker.predict(dev_cols_test))\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. The next step here is to do this for every permutation of the color context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinating Pragmatic Speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the Hallucinating Pragmatic Speaker to be the speaker that takes the k highest probability utterances that describes the context by the literal speaker, which then is filtered again by taking the top m number of utterances which maximize the literal listener likelihood of selecting the correct color.\n",
    "\n",
    "On a high level, the idea here is that the speaker is producing candidate utterances that it thinks is gramatically correct, while picking the top m utterances that maximizes understanding to the communicant. We will refer to this as utterances as hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_listener_hallucinations(input_colors, num_hallucinations=5, alpha=0.544, k_samples=10):\n",
    "    '''This method generates listener hallucinations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_colors:\n",
    "        A list of size (n,m,p) of int where each example has a list of m colors. Each color\n",
    "        is embedded in size p.\n",
    "    Returns\n",
    "    -------\n",
    "    prag_speaker_pred:\n",
    "        (n,k_samples,*) The top sentences from the speaker that maximizes the likelihood \n",
    "        that the listener will choose the target color. Each sentence can be of different\n",
    "        length and is tokenized.\n",
    "    '''\n",
    "    print(\"Sampling utterances\")\n",
    "    utterances = literal_speaker.sample_utterances(input_colors, k_samples=k_samples)\n",
    "    \n",
    "    print(\"Preparing Data\")\n",
    "    # Prepare data, flatten the target utterances and repeat the input colors per k_sample\n",
    "    target_utterances = [seq for seq_list in utterances for seq in seq_list]\n",
    "    input_colors_extended = [item for item in input_colors for i in range(k_samples)]\n",
    "    \n",
    "    print(\"Calculating probabilities\")\n",
    "    # utterance_preds = literal_listener.predict(input_colors_extended, target_utterances)\n",
    "    utterance_probs = literal_listener.predict(input_colors_extended, target_utterances, probabilities=True)\n",
    "    utterance_probs = torch.FloatTensor([preds[2] for preds in utterance_probs]).view(-1, k_samples)\n",
    "    utterance_probs = utterance_probs ** alpha\n",
    "    \n",
    "    total = torch.sum(utterance_probs, dim=1).unsqueeze(1)\n",
    "    normalized_utterance_probs = utterance_probs/total\n",
    "\n",
    "    print(\"Finding top m utterances\")\n",
    "    # Find the best k number of utterances that maximize the listener likelihood\n",
    "    best_utter_values, best_utter_indices = torch.topk(normalized_utterance_probs, num_hallucinations, dim=1)\n",
    "    \n",
    "    # DEPRECATED -Then flip the index number back.\n",
    "    # prag_speaker_pred_ind = normalized_utterance_probs.shape[1] - best_utter_index - 1\n",
    "    \n",
    "    # Index into the utterances to find the sequence candidates\n",
    "    prag_speaker_pred = [[seqs[utter_index] for utter_index in best_utter_indices[ind]] for ind, seqs in enumerate(utterances)]\n",
    "    return prag_speaker_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the input colors needed to predict for different candidate targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hallucinations = []\n",
    "for col_partition in [dev_cols_train[:10000], dev_cols_train[10000:20000], dev_cols_train[20000:30000], dev_cols_train[30000:]]:\n",
    "    torch.cuda.empty_cache()\n",
    "    third_col_speaker_pred = generate_listener_hallucinations(col_partition, num_hallucinations=5, k_samples=8)\n",
    "    top_hallucinations.append([seqs[0] for seqs in third_col_speaker_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hallucinations = [seq for seqs in top_hallucinations for seq in seqs]\n",
    "top_hallucinations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where each example has m candidate hallucinations.\n",
    "\n",
    "We can show that by taking the best hallucination produces a near 100% accuracy for the listener. This shows that the space of language that the speaker has learnt can perfectly solve the Stanford Colors problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_preds = literal_listener.predict(dev_cols_train, top_hallucinations)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these utterances perfectly capture the space of color differences? More needs to be done to examine this and is an excellent research direction.\n",
    "\n",
    "One other thing we can do is to train the speaker on these hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_speaker.warm_start = True\n",
    "# We only reassign the optimizer, not the graph.\n",
    "literal_speaker.opt = literal_speaker.optimizer(\n",
    "                literal_speaker.model.parameters(),\n",
    "                lr=literal_speaker.eta,\n",
    "                weight_decay=literal_speaker.l2_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_speaker.fit(dev_cols_train, top_hallucinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_preds_train = literal_speaker.predict(dev_cols_train)\n",
    "listened_preds = literal_listener.predict(dev_cols_train, speaker_preds_train)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_preds_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_preds_test = literal_speaker.predict(dev_cols_test)\n",
    "listened_preds = literal_listener.predict(dev_cols_test, speaker_preds_test)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_preds_test[1000:1420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (optional)\n",
    "def save_utts(utt_list, filename):\n",
    "    with open(filename, 'w') as filehandle:\n",
    "        for listitem in utt_list:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "#save_utts(speaker_preds_test, 'speaker_hallucinating_listener_with_attention.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listener-Hallicinating Speaker-based Pragmatic Listener "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_color_target = [[col_seq[2], col_seq[1],col_seq[0]] for col_seq in dev_cols_test]\n",
    "second_color_target = [[col_seq[0], col_seq[2],col_seq[1]] for col_seq in dev_cols_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_speaker_pred = generate_listener_hallucinations(first_color_target)\n",
    "second_col_speaker_pred = generate_listener_hallucinations(second_color_target)\n",
    "third_col_speaker_pred = generate_listener_hallucinations(dev_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_col_speaker_pred[:10])\n",
    "listened_preds = literal_listener.predict(second_color_target, [seqs[0] for seqs in second_col_speaker_pred])\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_col_speaker_pred[:10])\n",
    "listened_preds = literal_listener.predict(first_color_target, [seqs[0] for seqs in first_col_speaker_pred])\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this to normalize the test utterances and use bayesian inference to select the right target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hallucinations = 5\n",
    "alpha = 1.\n",
    "def find_utt_marginal(color_list, speaker_preds, alpha=0.01):\n",
    "    probs_per_col_context = []\n",
    "    total_over_all_utt = 0\n",
    "    for col_target, speaker_pred in [*zip(color_list, speaker_preds)]:\n",
    "        # Flatten the utterances\n",
    "        target_utterances = [seq for seq_list in speaker_pred for seq in seq_list]\n",
    "        input_colors_extended = [item for item in col_target for i in range(num_hallucinations)]\n",
    "        # Test all the utterances\n",
    "        #print(len(target_utterances), len(input_colors_extended))\n",
    "        lit_preds_per_col_context = torch.FloatTensor(literal_listener.predict(input_colors_extended, target_utterances, probabilities=True))\n",
    "        # Reshape the utterances and only take the prediction of the target\n",
    "        probs_per_col_context.append(lit_preds_per_col_context.view(-1, num_hallucinations, 3)[:, :, 2] ** alpha)\n",
    "        total_over_all_utt += torch.sum(probs_per_col_context[-1], dim=1)\n",
    "    return total_over_all_utt, probs_per_col_context\n",
    "    \n",
    "# Find both the marginal over utterances and the probability predictions per color context\n",
    "color_list = [first_color_target, second_color_target, dev_cols_test]\n",
    "speaker_preds = [first_col_speaker_pred, second_col_speaker_pred, third_col_speaker_pred]\n",
    "\n",
    "total_over_all_utt, lit_preds_per_col_context = find_utt_marginal(color_list, speaker_preds, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_probs = []\n",
    "# Now, we calculate the probabilities of the predictions\n",
    "for ind, col_target, speaker_pred in [*zip(range(3), color_list, speaker_preds)]:\n",
    "    test_pred = torch.FloatTensor(literal_listener.predict(col_target, dev_seqs_test, probabilities=True))\n",
    "    test_pred = test_pred[:, 2] ** alpha\n",
    "    \n",
    "    target_prior = torch.sum(lit_preds_per_col_context[ind], dim=1)\n",
    "    totals = total_over_all_utt + test_pred\n",
    "    test_preds_probs.append(test_pred/target_prior)\n",
    "test_preds_probs = torch.stack(test_preds_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = torch.argmax(test_preds_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum([1 if x == 2 else 0 for x in test_preds])\n",
    "print(\"test\", correct, \"/\", len(test_preds), correct/len(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the examples that were incorrect and see if we can analyze what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 200\n",
    "for i, x in enumerate(test_preds):\n",
    "    if x != 2:\n",
    "        print(third_col_speaker_pred[i][0], dev_seqs_test[i], x, i)\n",
    "        limit -= 1\n",
    "    if limit == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
