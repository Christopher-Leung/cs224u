{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: pragmatic color descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Leung\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [colors_overview.ipynb](colors_overview.ipynb) for set-up in instructions and other background details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_color_selector import (\n",
    "    ColorizedNeuralListener, create_example_dataset)\n",
    "from torch_color_describer import ColorizedInputDescriber\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All two-word examples as a dev corpus\n",
    "\n",
    "So that you don't have to sit through excessively long training runs during development, I suggest working with the two-word-only subset of the corpus until you enter into the late stages of system testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME, \n",
    "    word_count=None, \n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = list(dev_corpus.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subset has about one-third the examples of the full corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We __should__ worry that it's not a fully representative sample. Most of the descriptions in the full corpus are shorter, and a large proportion are longer. So this dataset is mainly for debugging, development, and general hill-climbing. All findings should be validated on the full dataset at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev dataset\n",
    "\n",
    "Let's load the saved training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pickle():\n",
    "    import pickle \n",
    "    \n",
    "    with open('dev_vocab.pickle', 'rb') as handle:\n",
    "        dev_vocab = pickle.load(handle)\n",
    "    with open('dev_seqs_test.pickle', 'rb') as handle:\n",
    "        dev_seqs_test = pickle.load(handle)\n",
    "    with open('dev_seqs_train.pickle', 'rb') as handle:\n",
    "        dev_seqs_train = pickle.load(handle)\n",
    "    with open('dev_cols_test.pickle', 'rb') as handle:\n",
    "        dev_cols_test = pickle.load(handle)\n",
    "    with open('dev_cols_train.pickle', 'rb') as handle:\n",
    "        dev_cols_train = pickle.load(handle)\n",
    "    with open('embedding.pickle', 'rb') as handle:\n",
    "        embedding = pickle.load(handle)\n",
    "    return dev_vocab, dev_seqs_test, dev_seqs_train, dev_cols_test, dev_cols_train, embedding\n",
    "dev_vocab, dev_seqs_test, dev_seqs_train, dev_cols_test, dev_cols_train, embedding = load_from_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our preprocessing steps are complete, and we can fit a first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embeddings\n",
    "\n",
    "We also load the GloVe embedding that was used by the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_pickle():\n",
    "    import pickle \n",
    "    with open('dev_glove_vocab.pickle', 'rb') as handle:\n",
    "        dev_glove_vocab = pickle.load(handle)\n",
    "    with open('dev_glove_embedding.pickle', 'rb') as handle:\n",
    "        dev_glove_embedding = pickle.load(handle)\n",
    "    return dev_glove_vocab, dev_glove_embedding\n",
    "dev_glove_vocab, dev_glove_embedding = load_glove_from_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above might dramatically change your vocabulary, depending on how many items from your vocab are in the Glove space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Literal Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "literal_listener = ColorizedNeuralListener(\n",
    "    dev_vocab, \n",
    "    #embedding=dev_glove_embedding, \n",
    "    embed_dim=100,\n",
    "    embedding=embedding,\n",
    "    hidden_dim=100, \n",
    "    max_iter=100,\n",
    "    batch_size=256,\n",
    "    dropout_prob=0.,\n",
    "    eta=0.001,\n",
    "    lr_rate=0.96,\n",
    "    warm_start=True,\n",
    "    device='cuda')\n",
    "literal_listener.load_model(\"literal_listener_with_attention.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_selector.py:77: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n"
     ]
    }
   ],
   "source": [
    "test_preds = literal_listener.predict(dev_cols_test, dev_seqs_test)\n",
    "train_preds = literal_listener.predict(dev_cols_train, dev_seqs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 9405 / 11749 0.8004936590348115\n",
      "train 31783 / 35245 0.901773301177472\n"
     ]
    }
   ],
   "source": [
    "correct = sum([1 if x == 2 else 0 for x in test_preds])\n",
    "print(\"test\", correct, \"/\", len(test_preds), correct/len(test_preds))\n",
    "correct = sum([1 if x == 2 else 0 for x in train_preds])\n",
    "print(\"train\", correct, \"/\", len(train_preds), correct/len(train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Literal Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "literal_speaker = ColorizedInputDescriber(\n",
    "    dev_glove_vocab, \n",
    "    embedding=dev_glove_embedding, \n",
    "    hidden_dim=100, \n",
    "    max_iter=40, \n",
    "    eta=0.005,\n",
    "    batch_size=128)\n",
    "literal_speaker.load_model(\"literal_speaker.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:70: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n",
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:677: RuntimeWarning: divide by zero encountered in power\n",
      "  perp = [np.prod(s)**(-1/len(s)) for s in scores]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8149629755723892"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_speaker.listener_accuracy(dev_cols_test, dev_seqs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinating Pragmatic Speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the Hallucinating Pragmatic Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_listener_hallucinations(input_colors, num_hallucinations=5, alpha=0.544, k_samples=10):\n",
    "    '''This method generates listener hallucinations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_colors:\n",
    "        A list of size (n,m,p) of int where each example has a list of m colors. Each color\n",
    "        is embedded in size p.\n",
    "    Returns\n",
    "    -------\n",
    "    prag_speaker_pred:\n",
    "        (n,k_samples,*) The top sentences from the speaker that maximizes the likelihood \n",
    "        that the listener will choose the target color. Each sentence can be of different\n",
    "        length and is tokenized.\n",
    "    '''\n",
    "    print(\"Sampling utterances\")\n",
    "    utterances = literal_speaker.sample_utterances(input_colors, k_samples=k_samples)\n",
    "    \n",
    "    print(\"Preparing Data\")\n",
    "    # Prepare data, flatten the target utterances and repeat the input colors per k_sample\n",
    "    target_utterances = [seq for seq_list in utterances for seq in seq_list]\n",
    "    input_colors_extended = [item for item in input_colors for i in range(k_samples)]\n",
    "    \n",
    "    print(\"Calculating probabilities\")\n",
    "    # utterance_preds = literal_listener.predict(input_colors_extended, target_utterances)\n",
    "    utterance_probs = literal_listener.predict(input_colors_extended, target_utterances, probabilities=True)\n",
    "    utterance_probs = torch.FloatTensor([preds[2] for preds in utterance_probs]).view(-1, k_samples)\n",
    "    utterance_probs = utterance_probs ** alpha\n",
    "    \n",
    "    total = torch.sum(utterance_probs, dim=1).unsqueeze(1)\n",
    "    normalized_utterance_probs = utterance_probs/total\n",
    "\n",
    "    print(\"Finding top m utterances\")\n",
    "    # Find the best k number of utterances that maximize the listener likelihood\n",
    "    best_utter_values, best_utter_indices = torch.topk(normalized_utterance_probs, num_hallucinations, dim=1)\n",
    "    \n",
    "    # DEPRECATED -Then flip the index number back.\n",
    "    # prag_speaker_pred_ind = normalized_utterance_probs.shape[1] - best_utter_index - 1\n",
    "    \n",
    "    # Index into the utterances to find the sequence candidates\n",
    "    prag_speaker_pred = [[seqs[utter_index] for utter_index in best_utter_indices[ind]] for ind, seqs in enumerate(utterances)]\n",
    "    return prag_speaker_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the input colors needed to predict for different candidate targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling utterances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:466: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n",
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n",
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n",
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n"
     ]
    }
   ],
   "source": [
    "top_hallucinations = []\n",
    "for col_partition in [dev_cols_train[:10000], dev_cols_train[10000:20000], dev_cols_train[20000:30000], dev_cols_train[30000:]]:\n",
    "    torch.cuda.empty_cache()\n",
    "    third_col_speaker_pred = generate_listener_hallucinations(col_partition, num_hallucinations=5, k_samples=8)\n",
    "    top_hallucinations.append([seqs[0] for seqs in third_col_speaker_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'grey', '</s>'],\n",
       " ['<s>', 'green', '</s>'],\n",
       " ['<s>', 'red', '</s>'],\n",
       " ['<s>', 'dull', 'blue', '</s>'],\n",
       " ['<s>', 'dark', '+est', 'green', '+ish', '</s>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hallucinations = [seq for seqs in top_hallucinations for seq in seqs]\n",
    "top_hallucinations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 34960 / 35245 0.9919137466307277\n"
     ]
    }
   ],
   "source": [
    "listened_preds = literal_listener.predict(dev_cols_train, top_hallucinations)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these utterances perfectly capture the space of color differences? More needs to be done to examine this and is an excellent research direction.\n",
    "\n",
    "One other thing we can do is to train the speaker on these hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_speaker.warm_start = True\n",
    "# We only reassign the optimizer, not the graph.\n",
    "literal_speaker.opt = literal_speaker.optimizer(\n",
    "                literal_speaker.model.parameters(),\n",
    "                lr=literal_speaker.eta,\n",
    "                weight_decay=literal_speaker.l2_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40; err = 23.515129979699856"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColorizedInputDescriber(\n",
       "\thidden_dim=100,\n",
       "\tbatch_size=128,\n",
       "\tmax_iter=40,\n",
       "\teta=0.005,\n",
       "\toptimizer=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tembed_dim=100,\n",
       "\tembedding=[[ 0.1394268  -0.47498924 -0.22497068 ...  0.02911435  0.47107838\n",
       "   0.3607797 ]\n",
       " [ 0.38472     0.49351     0.49096    ...  0.026263    0.39052\n",
       "   0.52217   ]\n",
       " [-0.66099    -0.073023    0.92379    ... -0.22556     0.8148\n",
       "  -0.44052   ]\n",
       " ...\n",
       " [ 0.4765     -0.14409    -0.49884    ... -1.1854     -0.88582\n",
       "  -0.57597   ]\n",
       " [-0.29881     0.81797     1.002      ... -0.23776    -0.90741\n",
       "   0.55244   ]\n",
       " [ 0.38433493  0.12163181  0.07975889 ... -0.19281575  0.35057666\n",
       "  -0.15122421]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_speaker.fit(dev_cols_train, top_hallucinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Github\\cs224u\\torch_color_describer.py:415: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  color_seqs = torch.FloatTensor(color_seqs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 34241 / 35245 0.9715136898850901\n"
     ]
    }
   ],
   "source": [
    "speaker_preds_train = literal_speaker.predict(dev_cols_train)\n",
    "listened_preds = literal_listener.predict(dev_cols_train, speaker_preds_train)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 11322 / 11749 0.9636564814026726\n"
     ]
    }
   ],
   "source": [
    "speaker_preds_test = literal_speaker.predict(dev_cols_test)\n",
    "listened_preds = literal_listener.predict(dev_cols_test, speaker_preds_test)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning about hallucinating speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Monroe et al.'s pragmatic speaker model based off RSA to construct the pragmatic hallucinating speaker.\n",
    "\n",
    "First, we generate permutations of the color context and feed them into the hallucinating speaker. Then, we do a prediction with the literal listener to find the probabilities of the utterances of each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_color_target = [[col_seq[2], col_seq[1],col_seq[0]] for col_seq in dev_cols_test]\n",
    "second_color_target = [[col_seq[0], col_seq[2],col_seq[1]] for col_seq in dev_cols_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n",
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n",
      "Finding top m utterances\n",
      "Sampling utterances\n",
      "Preparing Data\n",
      "Calculating probabilities\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.84 GiB (GPU 0; 8.00 GiB total capacity; 2.27 GiB already allocated; 1.03 GiB free; 5.21 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5c88f179a863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfirst_col_speaker_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_listener_hallucinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_color_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msecond_col_speaker_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_listener_hallucinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond_color_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mthird_col_speaker_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_listener_hallucinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_cols_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9fa2620fe10e>\u001b[0m in \u001b[0;36mgenerate_listener_hallucinations\u001b[1;34m(input_colors, num_hallucinations, alpha, k_samples)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calculating probabilities\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# utterance_preds = literal_listener.predict(input_colors_extended, target_utterances)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mutterance_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mliteral_listener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_colors_extended\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_utterances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mutterance_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mutterance_probs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mutterance_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterance_probs\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\cs224u\\torch_color_selector.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, color_seqs, word_seqs, probabilities, verbose, max_length)\u001b[0m\n\u001b[0;32m    990\u001b[0m                     \u001b[0mcolor_seqs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_colors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                     \u001b[0mword_seqs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m                     seq_lengths=batch_lens)\n\u001b[0m\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m                 \u001b[0mcolor_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\cs224u\\torch_listener_with_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, color_seqs, word_seqs, seq_lengths, mew, sigma)\u001b[0m\n\u001b[0;32m    102\u001b[0m             sigma=None):\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmew\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_seqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         output = self.decoder(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\cs224u\\torch_listener_with_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_seqs, seq_lengths)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Apply attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mattn_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matten_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.84 GiB (GPU 0; 8.00 GiB total capacity; 2.27 GiB already allocated; 1.03 GiB free; 5.21 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "first_col_speaker_pred = generate_listener_hallucinations(first_color_target)\n",
    "second_col_speaker_pred = generate_listener_hallucinations(second_color_target)\n",
    "third_col_speaker_pred = generate_listener_hallucinations(dev_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_preds = literal_listener.predict(second_color_target, first_color_preds)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_preds = literal_listener.predict(first_color_target, second_color_preds)\n",
    "correct = sum([1 if x == 2 else 0 for x in listened_preds])\n",
    "print(\"test\", correct, \"/\", len(listened_preds), correct/len(listened_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this to normalize over the best hallucinating utterances and use bayesian inference to select the right target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hallucinations = 5\n",
    "alpha = 1.\n",
    "def find_utt_marginal(color_list, speaker_preds, alpha=0.01):\n",
    "    probs_per_col_context = []\n",
    "    total_over_all_utt = 0\n",
    "    for col_target, speaker_pred in [*zip(color_list, speaker_preds)]:\n",
    "        # Flatten the utterances\n",
    "        target_utterances = [seq for seq_list in speaker_pred for seq in seq_list]\n",
    "        input_colors_extended = [item for item in col_target for i in range(num_hallucinations)]\n",
    "        # Test all the utterances\n",
    "        #print(len(target_utterances), len(input_colors_extended))\n",
    "        lit_preds_per_col_context = torch.FloatTensor(literal_listener.predict(input_colors_extended, target_utterances, probabilities=True))\n",
    "        # Reshape the utterances and only take the prediction of the target\n",
    "        probs_per_col_context.append(lit_preds_per_col_context.view(-1, num_hallucinations, 3)[:, :, 2] ** alpha)\n",
    "        total_over_all_utt += torch.sum(probs_per_col_context[-1], dim=1)\n",
    "    return total_over_all_utt, probs_per_col_context\n",
    "    \n",
    "# Find both the marginal over utterances and the probability predictions per color context\n",
    "color_list = [first_color_target, second_color_target, dev_cols_test]\n",
    "speaker_preds = [first_col_speaker_pred, second_col_speaker_pred, third_col_speaker_pred]\n",
    "\n",
    "total_over_all_utt, lit_preds_per_col_context = find_utt_marginal(color_list, speaker_preds, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_probs = []\n",
    "# Now, we calculate the probabilities of the predictions\n",
    "for ind, col_target, speaker_pred in [*zip(range(3), color_list, speaker_preds)]:\n",
    "    test_pred = torch.FloatTensor(literal_listener.predict(col_target, dev_seqs_test, probabilities=True))\n",
    "    test_pred = test_pred[:, 2] ** alpha\n",
    "    \n",
    "    target_prior = torch.sum(lit_preds_per_col_context[ind], dim=1)\n",
    "    totals = total_over_all_utt + test_pred\n",
    "    test_preds_probs.append(test_pred/target_prior)\n",
    "test_preds_probs = torch.stack(test_preds_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = torch.argmax(test_preds_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum([1 if x == 2 else 0 for x in test_preds])\n",
    "print(\"test\", correct, \"/\", len(test_preds), correct/len(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the examples that were incorrect and see if we can analyze what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 200\n",
    "for i, x in enumerate(test_preds):\n",
    "    if x != 2:\n",
    "        print(third_col_speaker_pred[i][0], dev_seqs_test[i], x, i)\n",
    "        limit -= 1\n",
    "    if limit == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
